Very terse deployment instructions for this webapp.

BMU has two components:

* A django webapp that allows users to upload media (via the
  "standard" Django file upload compents) and metadata (via a
  combination of form input and metadata in the EXIF data in the
  image)

* A batch component consisting of shell and python scripts, which take
  the images and metadata and "ingest" them into CSpace.

To deploy it on an IS&T Unix Team-managed server, (A RHEL6 VM, e.g. cspace-prod.cspace.berkeley.edu):

WEBAPP COMPONENT:

* Clone this repo and install and configure the Django
  webapp(s), including this one, "as usual".  In particular, create the temporary cache,
  with appropriate permissions, and record the full path in the deployed
  config file, uploadmedia.cfg

  e.g.

  [files]
  directory         = /tmp/image_upload_cache_pahma

BATCH COMPONENT:

* Follow the following steps to configure the batch component to run in
  the environment you are using: the batch component relies on cron or
  something similar to run, and has a number of expectations (dependencies)
  about where things are located and what they are named.
  Sorry it is so complicated, someday we'll make it more robust.

* Copy or create the appropriate .cfg file (from e.g. django_example_config) to a new config file in this directory
  (i.e. uploadmedia/) and edit the configuration for use by the the batch script postblob.sh,
  below.

  e.g.

  cp ~/django_example_config/pahma/pahma_Uploadmedia_Dev.cfg pahma_Uploadmedia_Dev.cfg
  vi pahma_Uploadmedia_Dev.cfg

* Copy postblobRevised.sh to postblob.sh in this directory. (NB postblobviaHttp.sh and postblobviaFile.sh are
  legacy version of this script, but they might be useful as they illustrate other ways to upload Blobs)

  cp postblobRevised.sh postblob.sh

* (NB: use postblobviaFile.sh if this webapp is running on the same server
  as the CSpace deployment it address -- and therefore can use the
  somewhat more efficient "direct file move" CSpace blob upload
  facility). Use postblobviaHttp.sh if the blob must be POSTed over HTTP to a
  remote CSpace server.

* This batch script does not require editing any more: all parameters are passed on the command line.

* Ensure that the temporary cache for images and intermediate files is
  present and properly configured with appropriate permissions. For the
  online portion, the current setting is /tmp/image_upload_cache_{TENANT}.

* Alas, there is no way to test this webapp except to upload some
  media files.

* There are some helper scripts: "runJob.sh" runs a single job from
  the command line. "runHelpers.sh" attempts to find problem media and
  create jobs to re-load/re-link them. See below.

* The batch portion of the system is run via cron, and apache owns the
  uploaded files so it has to be run as use apache.  And there is a
  script that reports on the status of uploads that runs via cron as
  well.

  Here is the part of the current crontab for app_webapps on cspace-prod.cspace, where the batch process run periodically:

```
(venv)[app_webapps@cspace-prod-01 ~]$ crontab  -l

# email bmu job stats
10  5  * * * perl /var/www/pahma/uploadmedia/checkRuns.pl jobs | expand -12 | mail -s "recent BMU jobs" pahma-cspace@berkeley.edu > /dev/null 2>&1
# run UCJEPS BMU (one minute after the hour, noon and 8pm)
1 12,20 * * * shopt -s nullglob; for f in /tmp/image_upload_cache_ucjeps/*.step1.csv; do f=$(echo $f | sed -e 's/\.step1.csv//') ; time /var/www/ucjeps/uploadmedia/postblob.sh ucjeps $f ucjeps_Uploadmedia_Prod >> /tmp/image_upload_cache_ucjeps/batches.log; done
```

A few more details on the batch process:

The "batch process", which runs nightly (or more frequently) as a cron job as shown above,
is really just a small bash script wrapper around a python script
(uploadMedia.py) that does the heavy lifting. The heavy lifting is as
follows:

* The webapp creates metadata files (*.step1.csv) in a tmp/ directory.
  These point to the media files that were uploaded via the webapp to
  the same tmp/ directory. (the directory is a config parameter to the
  webapps, and for PAHMA it currently points to
  dev.space:/tmp/image_upload)

* Every night, cron globs all the *.step1.csv files together, passes
  the list of files to uploadMedia.py, which POSTs to the Blob, Media and
  Relations services to create Blobs and Media records and connect them to the
  corresponding Collectionobjects (if needed).

* In more detail:

- the shell script postblobs.sh process a file (*.step1.csv) which is a
  list of image filenames and metadata. It calls the python script uploadMedia.py
  which POSTs to the Blob service to create blobs, then POSTs to the
  Media service, outputting a file *.step3.csv for every successful
  Media (and making bidirectional Relation if collectionobjects are involved.)

- Finally the bash script renames the *.step*.csv file to
  *.original.csv and *.processed.csv. This creates a trace for
  verification and prevents the images from being reprocessed the next
  time the cron job runs.

As an aside, note that this tmp/ directory is cleaned up by a script cleanBMUtempdir.sh,
which deletes files older than 48 hours -- i.e. there is a 2 day cache
of images load, in case something needs to be recovered or rerun. The various
"control files" (.csv and .log) are kept around forever.

Finally, there are a few other (very speculative!) scripts that are
used for reporting and maintenance:

* bulkmediaupload.sh - this runs a single *.step1.csv file; it is used
by the webapp to do "online uploads"

* checkObj.pl - this script checks Blobs uploaded vs. Media created; it
is used by the very rickety "runHelpers.sh" script below to report on
problem records.

* checkRuns.pl - this script creates some reports on BMU activity:
number of jobs and their status, lists of images and the CSIDs, finds
problem data (missing images, duplicates, etc.)

* runHelpers.sh - matches CSIDs loaded with log files, to find
discrepancies between what is in the database and what seems to have
been uploaded. No longer works due to schema changes in the Solr
datasource, which it uses to find Blob CSIDs.

* runJob.sh - like bulkmediaupload.sh, but does not cleanup/rename
intermediate files.

* verifyObjectsAndMedia.sh - somewhat scary script to make a list of
imagefile names and object numbers...

RUNNING THE BMU FROM THE COMMAND LINE

Once it is installed, the BMU can be invoked on the command line as:

$ bulkmediaupload.sh xxxxxx

Provided you have a metadata file named "xxxxxx.step1.csv" in the pwd with the following fields (the header is required):

name (i.e. field name)
size (can be null)
objectnumber
date (can be null)
creator (refname)
contributor
rightsholder (refname)

e.g.

1-4673_2014_10_13at09_18_25.JPG

4416952

1-4673

2014:10:13 09:18:25

urn:cspace:pahma.cspace.berkeley.edu:personauthorities:name(person):item:name(7652)'A Person Name'

Trinity Miller

urn:cspace:pahma.cspace.berkeley.edu:orgauthorities:name(organization):item:name(8107)'Phoebe A. Hearst Museum of Anthropology'

NB: the image files mentioned in this file must also be present in this same directory.
